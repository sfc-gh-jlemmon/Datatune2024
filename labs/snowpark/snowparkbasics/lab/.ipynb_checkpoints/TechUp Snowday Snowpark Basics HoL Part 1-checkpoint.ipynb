{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b648ee8",
   "metadata": {},
   "source": [
    "# Snowpark Basics HoL Part 1 - DataFrame Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed15903",
   "metadata": {},
   "source": [
    "## 1.1 Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2e8402-54a4-47da-a6b2-da3d063f934a",
   "metadata": {},
   "source": [
    "### Imports\n",
    "These imports are from our local Python environment, snowparkbasics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7410ce0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark.session import Session\n",
    "import snowflake.snowpark.functions as F\n",
    "import snowflake.snowpark.types as T\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Make sure we do not get line breaks when doing show on wide dataframes\n",
    "from IPython.core.display import HTML\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f95920",
   "metadata": {},
   "source": [
    "### Create Snowpark Session\n",
    "Using a credentials file simplifies the HoL but is not recommended as good practice for development or production environments.\n",
    "<br> The Python connector documentation explains how to use other authentication methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d5f942-4348-4763-93de-0d0e5f7ed4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('creds.json') as f:\n",
    "    connection_parameters = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49090e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = Session.builder.configs(connection_parameters).create()\n",
    "print(f\"Current Database and schema: {session.get_fully_qualified_current_schema()}\")\n",
    "print(f\"Current Warehouse: {session.get_current_warehouse()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0e003c-cf5c-4d1f-aa30-56da1ce2c296",
   "metadata": {},
   "source": [
    "### Modifying our Session\n",
    "We can use **session.sql** to issue any 'SQL' command. Note that due to lazy evaluation, typically nothing will happen without a show() or collect()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39be44fd-fd07-47c3-a546-2cbb00b3d551",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sql(\"USE WAREHOUSE TASTY_DEV_WH\").collect()\n",
    "print(f\"Current Warehouse: {session.get_current_warehouse()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84354485-ebe6-4220-aa88-3392468477c9",
   "metadata": {},
   "source": [
    "However, session also has a number of methods such as **use_warehouse()**. These *are* run immediately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62852de0-513e-4685-b1f2-c386293e4c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.use_warehouse(\"TASTY_DE_WH\")\n",
    "print(f\"Current Warehouse: {session.get_current_warehouse()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1c0c26",
   "metadata": {},
   "source": [
    "## 1.2 Loading a DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de7c8c2-5fc7-4ee7-9f55-6468179d7fee",
   "metadata": {},
   "source": [
    "### Pandas DataFrames from CSV\n",
    "Let's create a Pandas dataframe directly from csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6d1164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Pandas DataFrame - the order header csv is in fact the data from only one truck!!\n",
    "pandas_truck_df = pd.read_csv('data/truck.csv')\n",
    "pandas_header_df = pd.read_csv('data/header.csv')\n",
    "print(type(pandas_truck_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e71a5d8-269d-433e-be91-aef35aa342b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the Pandas dataframe\n",
    "pandas_truck_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e9da18-f43d-487f-b041-d5223b99395c",
   "metadata": {},
   "source": [
    "### Snowpark DataFrames from Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d391b0-58e0-4b39-90bb-27bb4ce78a22",
   "metadata": {},
   "source": [
    "The Snowpark **Table** class is a child of the **DataFrame** class.  We can define a dataframe based on a table very simply.\n",
    "<br> (We'll look at loading file data into tables in Part 3.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623695f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowpark_truck_df = session.table('TRUCK')\n",
    "snowpark_header_df = session.table('ORDER_HEADER')\n",
    "print(type(snowpark_truck_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01a48f9-ed44-4974-9493-137c303d328c",
   "metadata": {},
   "source": [
    "### Comparing DataFrames\n",
    "Compare sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d4f6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Size in MB of Pandas Truck DataFrame in Memory: ', np.round(sys.getsizeof(pandas_truck_df) / (1024.0**2), 2))\n",
    "print('Size in MB of Snowpark Truck DataFrame in Memory: ', np.round(sys.getsizeof(snowpark_truck_df) / (1024.0**2), 2))\n",
    "print('Size in MB of Pandas Header DataFrame in Memory: ', np.round(sys.getsizeof(pandas_header_df) / (1024.0**2), 2))\n",
    "print('Size in MB of Snowpark Header DataFrame in Memory: ', np.round(sys.getsizeof(snowpark_header_df) / (1024.0**2), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ea0fc5",
   "metadata": {},
   "source": [
    "The only thing stored in a Snowpark DataFrame is the SQL needed to return data.\n",
    "<br>Trying to manipulate even one truck's worth of order headers in Pandas starts to get 'interesting'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ac7d60-30d6-412b-baf8-b6010597e90c",
   "metadata": {},
   "source": [
    "Now, what is going on under the covers? You might want to log into your Snowflake account as the same user and review Snowsight Query History. But you can also use this DataFrame attribute from Snowpark..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43db7633",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowpark_header_df.queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cf44c3",
   "metadata": {},
   "source": [
    "A Snowpark DataFrame can be converted to a Pandas DataFrame. This will pull the data from Snowflake into the Python enviroment memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ec1ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_truck_df2 = snowpark_truck_df.to_pandas()\n",
    "pandas_truck_df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e707de",
   "metadata": {},
   "source": [
    "Both our Pandas DataFrames have the same shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db95baaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_truck_df.shape, pandas_truck_df2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327f42bc",
   "metadata": {},
   "source": [
    "### Displaying a Snowpark DataFrame\n",
    "Defining and modifying a Snowpark dataframe does not generally result in any activity within Snowflake - lazy evaluation, similar to Spark.\n",
    "The **show** method causes a query to be generated and sent and data returned - by default just 10 rows.\n",
    "In contrast **toPandas** or **to_pandas** will retrieve the whole dataset unless you set a **limit**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa02b8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowpark_header_df.show() # <- has a default limit of 10, and prints the data out\n",
    "snowpark_header_df.limit(5).toPandas() # <- collects first 5 rows and displays as pandas-dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93780fd5-3196-456e-8046-bf3dd1af2b33",
   "metadata": {},
   "source": [
    "In fact, you don't need to give your dataframe a name just to examine the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c628016-f381-47fd-8c75-950271d15cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.table(\"RAW_POS.LOCATION\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ea5929-551b-4c7c-826c-284bee7040fa",
   "metadata": {},
   "source": [
    "### Simple DataFrame Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accd4963-e678-4e0a-820f-4dc3c53cec37",
   "metadata": {},
   "source": [
    "The **count** method on a DataFrame will return the number of rows. This also triggers a query to Snowflake. *(Cf pyspark.sql.DataFrame.count())*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05786ed4-aaaa-4d7a-a03c-5758fdbff2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of rows in dataset\n",
    "snowpark_header_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde6eac9-5af8-46b0-b8a9-12f6d92bfacd",
   "metadata": {},
   "source": [
    "We can get an idea of the structure from the **schema** attribute.  *(Cf pyspark.sql.DataFrame.schema)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b67e604-0dbb-4b91-a90d-1d81c447f130",
   "metadata": {},
   "outputs": [],
   "source": [
    "header_schema = snowpark_header_df.schema\n",
    "header_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1c057b-8afb-4694-9e77-418c79e09694",
   "metadata": {},
   "source": [
    "Using the **describe** method will return some basic statistics for all numeric and string columns.  *(Cf pyspark.sql.DataFrame.describe())*\n",
    "<br>Note that this does real work inside Snowflake! The statistical values are not necessarily meaningful for all columns.\n",
    "<br>Can you find the maxiumum order value in the data? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4ae674-9c32-4472-9e4a-183f691be740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating various statistics per column\n",
    "snowpark_header_df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7848e9",
   "metadata": {},
   "source": [
    "## 1.3 Managing Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f43c9f-fc7f-46fe-b90a-ebac3a98d497",
   "metadata": {},
   "source": [
    "### Selecting Columns\n",
    "There are several ways to **select** specific columns, including **functions.col** and **DataFrame.col**. \n",
    "<br>The latter two are needed in several stuations to avoid ambiguities with string constants. \n",
    "<br>What do you notice about the four results below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a577f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "header_df1 = snowpark_header_df.select('ORDER_ID','TRUCK_ID','LOCATION_ID','ORDER_AMOUNT','ORDER_TS')\n",
    "header_df2 = snowpark_header_df[['ORDER_ID','TRUCK_ID','LOCATION_ID','ORDER_AMOUNT','ORDER_TS']] # -> pandas-like selection\n",
    "header_df3 = snowpark_header_df.select(F.col(\"ORDER_ID\"),F.col(\"truck_id\"),F.col(\"location_id\"),F.col(\"order_amount\"), F.col(\"order_ts\"))\n",
    "header_df4 = snowpark_header_df.select(snowpark_header_df.col('ORDER_ID'),snowpark_header_df.col('TRUCK_ID'),\n",
    "                                       snowpark_header_df.col('LOCATION_ID'),snowpark_header_df.col('ORDER_AMOUNT'),snowpark_header_df.col('ORDER_TS'))\n",
    "header_df1.show()\n",
    "header_df2.show()\n",
    "header_df3.show()\n",
    "header_df4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50703ac-a1f1-4961-b948-0f9109655ad5",
   "metadata": {},
   "source": [
    "In general in Python single and double quotes are interchangeable.  \n",
    "<br>Note that in all the examples above, the names are implicitly converted to uppercase.\n",
    "<br>To handle identifiers with lowercase you need to add explicit double quotes within the string, either as "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba089853-4704-4e59-a32b-546c925ab550",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following statement should fail\n",
    "snowpark_header_df.select(F.col('\"order_id\"'),F.col(\"TRUCK_ID\"),F.col(\"LOCATION_ID\"),F.col(\"ORDER_AMOUNT\"), F.col(\"ORDER_TS\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35149f79-6e0d-45af-a522-bf44538e6084",
   "metadata": {},
   "source": [
    "### Casting, Aliasing and In-Line Calculations\n",
    "We can **cast** the column datatypes. For example ORDER_AMOUNT could be cast to NUMBER(36,2). Alternatively we have functions like **to_date**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1580443c-7d5a-4ac4-9d57-821036f61f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "header_df1 = snowpark_header_df.select(F.col(\"ORDER_ID\"),F.col('ORDER_AMOUNT').cast(T.DecimalType(36,2)),F.to_date(F.col('ORDER_TS')))\n",
    "header_df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3df34c7-8ad1-4aeb-8bd1-2a75e6bac40d",
   "metadata": {},
   "source": [
    "That's a bit ugly. Let's alias those columns...  **alias**, **name** and **as_** all achieve the same effect. *(Cf pyspark alias or name)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a271172b-e139-461e-854a-0f92cefc7c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "header_df1 = snowpark_header_df.select(F.col(\"ORDER_ID\"),F.col('ORDER_AMOUNT').cast(T.DecimalType(36,2)).alias(\"ORDER_AMOUNT_2D\"),\n",
    "                                      F.to_date(F.col('ORDER_TS')).alias('ORDER_DATE'))\n",
    "header_df1.show()\n",
    "header_df1.queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67fc979",
   "metadata": {},
   "source": [
    "We can also include calculated expressions within a select as we can in SQL. For example we can use + - * / ** arithmetic operators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daf5ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "header_df2 = header_df1.select(F.col(\"ORDER_ID\"),F.col('ORDER_AMOUNT_2D'),\n",
    "                              (F.col('ORDER_AMOUNT_2D')*100).alias(\"OA_CENTS\"))\n",
    "header_df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c52a525-5ce5-4ad3-8f74-436976b53386",
   "metadata": {},
   "source": [
    "Again, switch to Snowsight Query History and see what is going on..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decf11ed-7630-4f18-a9df-e455cb92e125",
   "metadata": {},
   "source": [
    "### Adding and Removing Columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c88906",
   "metadata": {},
   "source": [
    "To add a new calculated column to a Snowpark DataFrame the **withColumn** or **with_column** method can be used.  *(Cf pysaprk withColumn)*\n",
    "In this example we are adding a new TRUCK column, AGE, that calculates the number of years since the YEAR. \n",
    "Note the use of F.col here - otherwise 'YEAR' could be seen as a string value. One approach is to use built-in Python functions to derive the current year locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e207ac8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from datetime import datetime\n",
    "year = datetime.now().year\n",
    "\n",
    "truck_df1 = snowpark_truck_df.select('TRUCK_ID','REGION','ISO_COUNTRY_CODE','YEAR','MAKE','MODEL','TRUCK_OPENING_DATE')\n",
    "truck_df1 = truck_df1.withColumn('AGE', year - F.col('YEAR'))\n",
    "truck_df1.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4eabd5",
   "metadata": {},
   "source": [
    "The following version pushes the current year 'calculation' down to Snowflake.\n",
    "\n",
    "In this section we show how each new version of the dataframe can replace the previous one by using the same name.\n",
    "This can make sense whilst we build up the dataframe query we really want. However, when we do this across cells and try to rerun just one cell we can get errors if a later statement has altered the structure that an earlier statement relied on.... We avoid that here by redefining truck_df1 from its source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9905ea-12c7-4e0f-9829-112c3633fae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "truck_df1 = snowpark_truck_df.select('TRUCK_ID','REGION','ISO_COUNTRY_CODE','YEAR','MAKE','MODEL','TRUCK_OPENING_DATE')\n",
    "truck_df1 = truck_df1.withColumn('AGE', F.date_part(\"year\", F.current_date()) - F.col('YEAR'))\n",
    "truck_df1.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c380a5-15dc-46b4-9733-4118d0f36f18",
   "metadata": {},
   "source": [
    "If we do not want to use specific columns we can use **drop** to remove those from a Snowpark DataFrame.  \n",
    "**Note:** This is not removing them from the underlying table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6057de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop a column\n",
    "truck_df1 = truck_df1.drop('MODEL','YEAR')\n",
    "truck_df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caed84d6-3778-42cc-8dbe-87f78c81fccd",
   "metadata": {},
   "source": [
    "## 1.4 Simple Data Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea617ddf",
   "metadata": {},
   "source": [
    "### Filtering Rows\n",
    "To filter/select specific rows we use **filter**.\n",
    "A whole set of column operators are available to be used e.g. \n",
    "\n",
    "==, !=, <, <=, >, >=  for comparisons;   &, |  and or;  + - * / **  arithmetic operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abe304d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data\n",
    "truck_df2 = truck_df1.filter(F.col('ISO_COUNTRY_CODE') == 'GB')\n",
    "truck_df2.show()\n",
    "truck_df3 = truck_df1.filter(F.col('ISO_COUNTRY_CODE').in_('ES','FR','GB')).sort('ISO_COUNTRY_CODE')\n",
    "truck_df3.show()\n",
    "truck_df4 = truck_df1.filter(F.col('ISO_COUNTRY_CODE').like('F%'))\n",
    "truck_df4.show()\n",
    "truck_df3.queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc21227",
   "metadata": {},
   "source": [
    "### Sorting\n",
    "We may want to see data in a specific order. For this the **sort** method is used..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2196027d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort data\n",
    "truck_df4 = truck_df4.sort(F.col('TRUCK_OPENING_DATE').desc(),F.col('ISO_COUNTRY_CODE'))\n",
    "truck_df4.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ad49ef-2ddb-4322-8ef3-c2c81e9ce1c8",
   "metadata": {},
   "source": [
    "### Aggregation\n",
    "To aggregate data the **groupBy** or **group_by** method is typically used. The groupby method produces a RelationalGroupedDataFrame object with its own specific methods, which, in turn, return a DataFrame. The **agg** method provides the most flexibility for managing the output, and including different aggregate metrics for different columns. Note the syntax - although operating on columns, the functions like avg expect the string of the column name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a56c822",
   "metadata": {},
   "outputs": [],
   "source": [
    "truck_df5 = truck_df3.groupBy(['ISO_COUNTRY_CODE','MAKE']).agg(\n",
    "             [F.count('*').alias('COUNT'),F.avg('AGE').alias('AVG_TRUCK_AGE'),F.max('TRUCK_ID').alias('MAX_TRUCK_ID')])\n",
    "truck_df5 = truck_df5.sort(F.col('ISO_COUNTRY_CODE'), F.col('COUNT').desc())\n",
    "truck_df5.queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7f204e-6443-4625-b1ab-c567addac20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "truck_df5.show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eceb2710-ff89-492a-b80c-032b897afbbe",
   "metadata": {},
   "source": [
    "### Using SQL\n",
    "How might we express the same combined query in SQL? It is quite likely that we would want to break it down in a similar way.\n",
    "We can run SQL queries directly using **session.sql** (including Snowflake commands issued as SQL). \n",
    "Note that nothing will happen without a collect() or show().\n",
    "\n",
    "In the example below, the three quotes beginning and end are how we indicate a multi-line string in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367ebc20-57d6-47f1-957a-fc095798cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "truck_df6 = session.sql(\"\"\"\n",
    " SELECT ISO_COUNTRY_CODE, MAKE, count(1) AS COUNT, avg(AGE) AS AVG_TRUCK_AGE, max(TRUCK_ID) AS MAX_TRUCK_ID \n",
    "   FROM ( SELECT TRUCK_ID, REGION, ISO_COUNTRY_CODE, MAKE, TRUCK_OPENING_DATE, (date_part('year', current_date()) - YEAR) AS AGE \n",
    "       FROM TRUCK WHERE ISO_COUNTRY_CODE IN ('ES', 'FR', 'GB')\n",
    "     ) \n",
    "   GROUP BY ISO_COUNTRY_CODE, MAKE ORDER BY ISO_COUNTRY_CODE ASC NULLS FIRST, COUNT DESC NULLS LAST LIMIT 15\n",
    "   \"\"\")\n",
    "truck_df6.show()\n",
    "truck_df6.queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3c3a6a",
   "metadata": {},
   "source": [
    "## 1.5 Persist Transformations\n",
    "\n",
    "If we want to save the changes we can either save it as a table, meaning the SQL generated by the DataFrame is executed and the result is stored in a table or as a view where the DataFrame SQL will be the definition of the view.  \n",
    "**save_as_table** saves the result in a table, if **mode='overwrite'** then it will also replace the data that is in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995c4a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "truck_df3.write.save_as_table(table_name='TRUCK_ANALYSIS', mode='overwrite')\n",
    "session.table('TRUCK_ANALYSIS').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b46de37-0442-4452-a64b-766dfc4c9914",
   "metadata": {},
   "source": [
    "## 1.X YOUR TURN!\n",
    "\n",
    "Here is the challenge: Generate a list of months for which we have data, the total order amount for each month (assume amounts are all held in the same currency), and the number of distinct locations visted in each month.\n",
    "<br>Hints:\n",
    "Functions you may find useful include **count_distinct** (aka countDistinct), **date_part**, **to_char** with numeric formatting '09' or 'FM09' and **concat**.\n",
    "\n",
    "### Hint:   \n",
    "To see all methods available use the TAB key.    F.<TAB>   Will show all functon methods.\n",
    "\n",
    "To see for a specific function the help text.   SHIFT-TAB\n",
    "                                                                                                                                                      \n",
    "https://docs.snowflake.com/developer-guide/snowpark/reference/scala/com/snowflake/snowpark/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e80c981",
   "metadata": {},
   "source": [
    "### Check out the data\n",
    "\n",
    "What columns in Order Header will you need?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e604fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f4eaceb-b960-4103-84b6-c96baf2ae849",
   "metadata": {},
   "source": [
    "### Select the columns you need and create a Month column\n",
    "You'll need to concatenate the year and month date parts of the order timestamp.\n",
    "Note that (currently) date_format in Snowpark is an alias of to_date and not the date to string functionality of Pyspark..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fd3a99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d5e7eb-988e-49a2-ac6e-55825fe5419f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc09aaf8",
   "metadata": {},
   "source": [
    "### Now aggregate by month\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dd7e50-4eb4-47e3-af53-3ca409bf74cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7287f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16aaa4a-d4f3-4cc4-ae8e-bea4cf1683d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a5bada-efaa-43ea-a5f7-3f8006c35bba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
