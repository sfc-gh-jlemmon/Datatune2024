{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b648ee8",
   "metadata": {},
   "source": [
    "# Snowpark Basics HoL Part 4 - Stored Procedures and Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed15903",
   "metadata": {},
   "source": [
    "## 4.1 Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2e8402-54a4-47da-a6b2-da3d063f934a",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7410ce0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark.session import Session\n",
    "import snowflake.snowpark.functions as F\n",
    "import snowflake.snowpark.types as T\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Make sure we do not get line breaks when doing show on wide dataframes\n",
    "from IPython.core.display import HTML\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f95920",
   "metadata": {},
   "source": [
    "### Create Snowpark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d5f942-4348-4763-93de-0d0e5f7ed4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('creds.json') as f:\n",
    "    connection_parameters = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49090e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = Session.builder.configs(connection_parameters).create()\n",
    "print(f\"Current Database and schema: {session.get_fully_qualified_current_schema()}\")\n",
    "print(f\"Current Warehouse: {session.get_current_warehouse()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e9da18-f43d-487f-b041-d5223b99395c",
   "metadata": {},
   "source": [
    "### Create Stages\n",
    "These will be used below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8556b65-7e3d-4d9b-8fcc-dcd0173eeb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sql('CREATE OR REPLACE STAGE PROCSTAGE').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40d0eff-a431-44c9-99de-db0a880619d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sql('CREATE OR REPLACE STAGE UDFSTAGE').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a9151a-5e2d-4ae6-aa59-61d301dfcd93",
   "metadata": {},
   "source": [
    "### Snowpark DataFrames from Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623695f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Snowpark DataFrame\n",
    "snowpark_truck_df = session.table('TRUCK')\n",
    "snowpark_header_df = session.table('ORDER_HEADER')\n",
    "snowpark_detail_df = session.table('ORDER_DETAIL')\n",
    "snowpark_location_df = session.table('LOCATION')\n",
    "snowpark_menu_df = session.table('MENU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01a48f9-ed44-4974-9493-137c303d328c",
   "metadata": {},
   "source": [
    "## 4.2 Using Stored Procedures\n",
    "Stored procedures may seem less natural to typical Python users, but they can be useful for several reasons:\n",
    "<br>They provide a way to capture and run a set of Python commands, potentially with parameterisation.\n",
    "They can include all sorts of complex Python logic, and access code and other files stored in Snowflake stages.\n",
    "They run server-side, so if they use functionality such as Pandas, or are training a model which takes place outside the SQL engine, the CPU and memory is server-side not client.\n",
    "\n",
    "One consideration is that procedures themselves run single node. However, if they invoke dataframe API processing including UDFs, that processing can parallelize across multiple nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f09c8a-d6e6-4160-8740-24d38e03b741",
   "metadata": {},
   "source": [
    "### Stored Procedure as Script\n",
    "Let's turn the Part 2 solution into a SP which takes a Menu Item Category, Year and Month and creates a summary table for that combination. \n",
    "\n",
    "First we define the Python function. Note that in this code the indentation becomes important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b047d89e-10ea-41f7-b20e-09b93af6905e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_sproc(session: Session,  category: str, selectyear: int, selectmonth: int)-> None:\n",
    "    # Define table based dataframes\n",
    "    header_df = session.table('RAW_POS.ORDER_HEADER').select(['ORDER_ID', 'LOCATION_ID', 'ORDER_TS'])\n",
    "    detail_df = session.table('RAW_POS.ORDER_DETAIL').select(['ORDER_DETAIL_ID','ORDER_ID','MENU_ITEM_ID','QUANTITY'])\n",
    "    location_df = session.table('RAW_POS.LOCATION').select(['LOCATION_ID','ISO_COUNTRY_CODE'])\n",
    "    menu_df = session.table('RAW_POS.MENU').select(['MENU_ITEM_ID','MENU_ITEM_NAME','ITEM_CATEGORY'])\n",
    "\n",
    "    # Filter based on inputs\n",
    "    header_df = header_df.filter((F.year(F.col('ORDER_TS'))== selectyear ) & (F.month(F.col('ORDER_TS'))==selectmonth))\n",
    "    menu_df = menu_df.filter(F.col('ITEM_CATEGORY') == category) \n",
    "    \n",
    "    # Combine and aggregate\n",
    "    combined_df = detail_df.join(header_df,\"ORDER_ID\").join(menu_df,\"MENU_ITEM_ID\").join(location_df,\"LOCATION_ID\")\n",
    "    output_df = combined_df.groupBy(['MENU_ITEM_ID', 'MENU_ITEM_NAME', 'ISO_COUNTRY_CODE']).agg(F.sum('QUANTITY').alias('TOTAL_QUANTITY'))\n",
    "    \n",
    "    # Set up table name and write to table\n",
    "    outputtable = category.upper() + str(selectyear) + str(selectmonth).zfill(2)\n",
    "    output_df.write.save_as_table(table_name=outputtable, mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96508246-6a5a-44ed-853a-345189832083",
   "metadata": {},
   "source": [
    "Next we register the function as a stored procedure. Note that for a permanent stored procedure you must name a stage. \n",
    "(You may want to watch this in Snowsight Query History.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfed441-0de3-48c5-93ee-c93abc79a12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_sproc = session.sproc.register(\n",
    "    func=category_sproc, # the name of the function (see cell above) \n",
    "    name='category_sproc', # the name of the function once stored in SF\n",
    "    is_permanent=True, # store it permanently?\n",
    "    replace=True, # replace anything that was there under this name\n",
    "    stage_location='@PROCSTAGE', # the stage where we store it\n",
    "    packages=['snowflake-snowpark-python'],) # the packages the function uses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1659e1-6490-4b46-b4bb-06bbd6b2647f",
   "metadata": {},
   "source": [
    "Finally we call the procedure. Again you may want to try this directly inside Snowsight from SQL - just ignore the session parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b050a92-177f-44ed-a096-862b0d5ffb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_sproc(session, 'Dessert', 2022, 4)\n",
    "session.table('DESSERT202204').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48d031a-8894-417a-8dc7-3acdc62db422",
   "metadata": {},
   "source": [
    "## 4.3 Using Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5dbf11-5c9e-4f0e-b2a8-292f02ff148b",
   "metadata": {},
   "source": [
    "### Calling Built-In Functions\n",
    "While many standard built-in functions in Snowflake have matching Snowpark **functions** methods, given the rate at which Snowflake adds new functionality, there will always be functions you might need outside that list.\n",
    "<br>For these you can use **functions.call_builtin** or **functions.call_function**.  \n",
    "\n",
    "Let's use this to try a simple geospatial function. We'll start from a different Tasty Bytes view which brings together enhanced location data and cross-join it with itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5a29c3-badb-4538-a7a0-4b445ac888e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_df1 = session.table('ANALYTICS.LOCATION_DETAIL_V').select('LOCATION_ID','LOCATION_NAME', 'LATITUDE', 'LONGITUDE', 'CITY')\\\n",
    "                        .filter(F.col('CITY') == 'Paris')\n",
    "location_df1.show(5)\n",
    "location_df2 = location_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a0bfb0-4f42-4dec-a4b7-eaee41411195",
   "metadata": {},
   "outputs": [],
   "source": [
    "locationpairs_df = location_df1.cross_join(location_df2, lsuffix='_DF1',rsuffix='_DF2').filter(F.col('LOCATION_ID_DF1') > F.col('LOCATION_ID_DF2'))\n",
    "locationpairs_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b747bb-f0d1-4ab8-8f47-c31b41641ef3",
   "metadata": {},
   "source": [
    "Now let's invoke the ST_MAKEPOINT and ST_DISTANCE functions to find the proximity between locations, and then display the locations closest to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5872212-488d-4bac-9a88-bad778b31bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "locationdist_df = locationpairs_df.select(F.col('LOCATION_NAME_DF1'), F.col('LOCATION_NAME_DF2'), \n",
    "                    F.call_builtin('ST_MAKEPOINT',F.col('LONGITUDE_DF1'), F.col('LATITUDE_DF1')).alias('GEO_POINT_DF1'),\n",
    "                    F.call_builtin('ST_MAKEPOINT',F.col('LONGITUDE_DF2'), F.col('LATITUDE_DF2')).alias('GEO_POINT_DF2'))\n",
    "locationdist_df = locationdist_df.select(F.col('LOCATION_NAME_DF1'), F.col('LOCATION_NAME_DF2'), \n",
    "                     (F.call_builtin('ST_DISTANCE',F.col('GEO_POINT_DF1'), F.col('GEO_POINT_DF2'))/1000).alias('DISTANCE_KM'))\n",
    "locationdist_df = locationdist_df.select(['LOCATION_NAME_DF1', 'LOCATION_NAME_DF2','DISTANCE_KM']).sort(F.col('DISTANCE_KM').desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae0bb7a-575d-4e09-8e9b-833ed7179891",
   "metadata": {},
   "source": [
    "However, we can simplify this further. The **functions.function** method provides a way to locally name that function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60355199-59e4-4269-b2f4-a44dd272f042",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_makepoint = F.function('ST_MAKEPOINT')\n",
    "geo_distance = F.function('ST_DISTANCE')\n",
    "\n",
    "locationdist_df = locationpairs_df.select(F.col('LOCATION_NAME_DF1'), F.col('LOCATION_NAME_DF2'), \n",
    "                    geo_makepoint(F.col('LONGITUDE_DF1'), F.col('LATITUDE_DF1')).alias('GEO_POINT_DF1'),\n",
    "                    geo_makepoint(F.col('LONGITUDE_DF2'), F.col('LATITUDE_DF2')).alias('GEO_POINT_DF2'))\n",
    "locationdist_df = locationdist_df.select(F.col('LOCATION_NAME_DF1'), F.col('LOCATION_NAME_DF2'), \n",
    "                     (geo_distance(F.col('GEO_POINT_DF1'), F.col('GEO_POINT_DF2'))/1000).alias('DISTANCE_KM'))\n",
    "locationdist_df = locationdist_df.select(['LOCATION_NAME_DF1', 'LOCATION_NAME_DF2','DISTANCE_KM']).sort(F.col('DISTANCE_KM').desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249f72e2-e64a-4a43-9174-0ce8b0352a4b",
   "metadata": {},
   "source": [
    "### Existing UDFs\n",
    "Let's capture the distance calculation above in a SQL UDF.  Note that both SQL and Python UDFs can be defined in 'SQL' CREATE FUNCTION."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de01ae3-40cc-4df9-9af8-b9ac80bb1b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sql(\"\"\"\n",
    "            create or replace function longlatdistance (long1 float, lat1 float, long2 float, lat2 float) returns float\n",
    "            as \n",
    "            'st_distance(st_makepoint(long1, lat1),st_makepoint(long2, lat2))' \n",
    "            \"\"\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ea0fc5",
   "metadata": {},
   "source": [
    "Now we can call that udf using **functions.call_udf**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23c1af7-800f-4415-bb90-e7265785bbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "locationdist_df = locationpairs_df.select(F.col('LOCATION_NAME_DF1'), F.col('LOCATION_NAME_DF2'),\n",
    "            (F.call_udf(\"longlatdistance\", F.col('LONGITUDE_DF1'), F.col('LATITUDE_DF1'),F.col('LONGITUDE_DF2'), F.col('LATITUDE_DF2'))/1000)\n",
    "                .alias('DISTANCE_KM'))\n",
    "locationdist_df.sort(F.col('DISTANCE_KM').desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31201e41-71bc-4fe2-8f6b-09a9f038406c",
   "metadata": {},
   "source": [
    "## 4.4 Creating Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abea7a66-466e-45a8-ba1b-e847f87be8a3",
   "metadata": {},
   "source": [
    "### Creating a Python UDF in SQL\n",
    "We can define a Python UDF inline within SQL. This code could also be run in a Snowsight SQL Worksheet.\n",
    "<br>(Note that in the real world this very simple example, which could be run in a SQL UDF, is not necessarily a good use of Python UDF capabilities, as it forces the data through the Python UDF, whereas the optimizer can 'extract' and combine SQL from a SQL UDF with other SQL Clauses.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda74c6c-2506-4f8a-ac4d-347949ef41f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sql(\"\"\"\n",
    "CREATE OR REPLACE FUNCTION profit_margin(\n",
    " cost decimal(38,4)\n",
    ",sale decimal(38,4)\n",
    "           )\n",
    "returns decimal(38,2) not null\n",
    "language python\n",
    "runtime_version = '3.9'\n",
    "handler = 'profit_margin'\n",
    "as\n",
    "$$    \n",
    "import decimal\n",
    "def profit_margin(\n",
    "    cost: decimal.Decimal\n",
    "  , sale: decimal.Decimal\n",
    "    ):\n",
    "    if cost != 0:\n",
    "        return round(((sale - cost)/cost)*100,2)\n",
    "    else:\n",
    "        return 0\n",
    "$$\n",
    ";\n",
    "\n",
    "            \"\"\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b574507b-1cd2-4cf8-ba04-14c29e7a6c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "menu_df = snowpark_menu_df.select(F.col('MENU_ID'), F.col('COST_OF_GOODS_USD'), F.col('SALE_PRICE_USD'), \n",
    "                                  F.call_udf('profit_margin', F.col('COST_OF_GOODS_USD'), F.col('SALE_PRICE_USD')).alias('PROFIT_MARGIN'))\n",
    "menu_df.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b21a2a-7f4e-41bc-b743-b496fa412ce6",
   "metadata": {},
   "source": [
    "### Creating a Python UDF in Snowpark\n",
    "We can also define the UDF in Snowpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a06dc02-2803-41dd-bdb9-0aa88c19b222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Python function locally\n",
    "import decimal\n",
    "def profit_margin(\n",
    "    cost: decimal.Decimal\n",
    "  , sale: decimal.Decimal\n",
    "    ):\n",
    "    if cost != 0:\n",
    "        return round(((sale - cost)/cost)*100,2)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21257bd-8522-47e4-8d8d-3f647683436b",
   "metadata": {},
   "source": [
    "Then we need to register the UDF. \n",
    "<br>Note that we need to use the types from snowflake.snowpark.types, and that input_types expects a list, even if it contains just one element.\n",
    "<br>The stage location has to be provided, but if the generated code is small enough to be stored inline in metadata, you may see nothing added to the stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f4c45e-b3a0-474c-a9ff-4bfe0a0f4f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload UDF to Snowflake\n",
    "session.udf.register(\n",
    "    func = profit_margin\n",
    "  , return_type = T.DecimalType(38,2)\n",
    "  , input_types = [T.DecimalType(38,4), T.DecimalType(38,4)]\n",
    "  , is_permanent = True\n",
    "  , name = 'profit_margin'\n",
    "  , replace = True\n",
    "  , stage_location = '@UDFSTAGE'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7d6a0d-2d06-4731-a822-ac90b106759a",
   "metadata": {},
   "outputs": [],
   "source": [
    "menu_df = snowpark_menu_df.select(F.col('MENU_ID'), F.col('COST_OF_GOODS_USD'), F.col('SALE_PRICE_USD'), \n",
    "                                  F.call_udf('profit_margin', F.col('COST_OF_GOODS_USD'), F.col('SALE_PRICE_USD')).alias('PROFIT_MARGIN'))\n",
    "menu_df.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea112b7-cd22-4acc-bd54-f7691c082a17",
   "metadata": {},
   "source": [
    "### Using the @udf Decorator\n",
    "Finally, we can do the same using a Python decorator approach. You may see this in code examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6acfd9c-8c96-43c6-a048-5686e14a2bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import decimal\n",
    "from snowflake.snowpark.functions import udf\n",
    "\n",
    "@udf(return_type = T.DecimalType(38,2), input_types = [T.DecimalType(38,4), T.DecimalType(38,4)], \n",
    "     is_permanent = True, name = \"profit_margin\", replace = True, stage_location = '@UDFSTAGE', session=session)\n",
    "def profit_margin(cost: decimal.Decimal, sale: decimal.Decimal) -> decimal.Decimal:\n",
    "    if cost != 0:\n",
    "        return round(((sale - cost)/cost)*100,2)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870844e2-93e8-4ffa-a67b-c6d3640983f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "menu_df = snowpark_menu_df.select(F.col('MENU_ID'), F.col('COST_OF_GOODS_USD'), F.col('SALE_PRICE_USD'), \n",
    "                                  F.call_udf('profit_margin', F.col('COST_OF_GOODS_USD'), F.col('SALE_PRICE_USD')).alias('PROFIT_MARGIN'))\n",
    "menu_df.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c65736-e1d6-48ae-abfc-a729991f20fd",
   "metadata": {},
   "source": [
    "### Vectorised UDFs\n",
    "If you examine the Query Profile of the previous query, you will see that the UF appears as an Extension Function, and there are some Statistics including Total Python UDF handler invocations: 20 and Total Python UDF rows processed: 20\n",
    "<br>In this case, the ms timings are tiny, but if we were processing large numbers of rows, or even using a Pandas library designed to process batches of data efficiently, we can set up the UDF to run with batches at a time, using the Pandas API.\n",
    "<br>Note that this example uses a lambda function to bypass decimal division by zero issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690ab7f5-99d0-49b7-a02b-9dd24af76a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import decimal\n",
    "from snowflake.snowpark.functions import udf\n",
    "\n",
    "@udf(return_type = T.DecimalType(38,2), input_types = [T.DecimalType(38,4), T.DecimalType(38,4)], \n",
    "     is_permanent = True, name = \"profit_margin_batch\", replace = True, stage_location = '@UDFSTAGE', session=session)\n",
    "def profit_margin_batch1(pdf: T.PandasDataFrame [decimal.Decimal, decimal.Decimal]) -> T.PandasSeries[decimal.Decimal]:\n",
    "    pdf.columns = [\"cost\", \"sale\"]\n",
    "    pdf[\"result\"] = (pdf.apply(lambda x: (x['sale'] - x['cost']) / x['cost'] if x['cost'] != 0 else decimal.Decimal('NaN'), axis=1))*100\n",
    "    return pdf[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cdb885-40a6-4c87-a2d0-7aa549f12c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "menu_df = snowpark_menu_df.select(F.col('MENU_ID'), F.col('COST_OF_GOODS_USD'), F.col('SALE_PRICE_USD'), \n",
    "                                  F.call_udf('profit_margin_batch', F.col('COST_OF_GOODS_USD'), F.col('SALE_PRICE_USD')).alias('PROFIT_MARGIN'))\n",
    "menu_df.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb2aa72-572a-40d7-82c2-421809f6ae19",
   "metadata": {},
   "source": [
    "If you examine the Query Profile of the previous query, you will see that the UDF appears as an Extension Function, and there are some Statistics including Total Python UDF handler invocations: 1 and Total Python UDF rows processed: 20.\n",
    "<br> If we wanted to process all the rows the number of rows per invocation would be even higher. You can set a maximum batch size to avoid the UDF invocation timing out at 60 seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec879589-91f8-4dda-ba71-4446b71a3a52",
   "metadata": {},
   "source": [
    "## 4.X YOUR TURN!\n",
    "\n",
    "Here is the challenge: \n",
    "<br>You realise that the line in the solution to part 1\n",
    "<br>`F.concat(F.to_char(F.date_part(\"year\",'ORDER_TS')), F.to_char(F.date_part(\"month\",'ORDER_TS'),'FM09'))`\n",
    "<br>could also be written generically in Python as:\n",
    "<br>`return str(ts.year) + str(ts.month)`\n",
    "where ts is the datetime type.  \n",
    "<br>(If that was already part of your solution to Part 1, congratulations!)\n",
    "<br>Create and register a Python UDF char_month to implement this and reproduce the answer to Part 1 using this. Start by separately defining a function and registering it. Then move on to decorators and vectorized UDFs if you wish...\n",
    "<br>Hint: you will need to import datetime from datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f52ca57-4653-483a-96ed-095a102a71bb",
   "metadata": {},
   "source": [
    "### Define the function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa2e829-4e3e-4c92-8d33-2300d4bb1347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Python function locally\n",
    "from datetime import datetime\n",
    "def char_month(\n",
    "    ts: datetime\n",
    "    ):\n",
    "    return str(ts.year) + str(ts.month)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba26309-0fd5-42f3-a450-2ba5ab6ffa48",
   "metadata": {},
   "source": [
    "### Register the function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ecb100-936c-4d8c-8ee5-b03ab7606eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark.types import StringType, TimestampType\n",
    "\n",
    "# Upload UDF to Snowflake\n",
    "session.udf.register(\n",
    "    func = char_month\n",
    "  , return_type = T.StringType()\n",
    "  , input_types = [T.TimestampType()]\n",
    "  , is_permanent = True\n",
    "  , name = 'char_month'\n",
    "  , replace = True\n",
    "  , stage_location = '@UDFSTAGE'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ac4d5f-b306-40ea-9022-87ae55a75e99",
   "metadata": {},
   "source": [
    "### Test with Order Header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c551a8-9f8d-4e8b-85f5-f958265327d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "exercise_header_df = session.table(\"ORDER_HEADER\")\n",
    "header_df1 = exercise_header_df.select(F.col('ORDER_ID'), F.col('LOCATION_ID'), \n",
    "        F.col('ORDER_AMOUNT').cast(T.DecimalType(36,2)).alias(\"ORDER_AMOUNT\"),\n",
    "        F.call_udf('char_month',F.col('ORDER_TS')).alias('ORDER_MONTH'))\n",
    "header_df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80f35d2-1ea0-4816-8923-e474cdeacaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96a65b7-e7e3-4164-ab6b-5c046a5a475a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
